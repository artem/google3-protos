syntax = "proto3";

package stream_executor;

import "third_party/tensorflow/compiler/xla/autotune_results.proto";

message CudaComputeCapabilityProto {
    int32 major = 1;
    int32 minor = 2;
}

message RocmComputeCapabilityProto {
    string gcn_arch_name = 1;
}

message GpuDeviceInfoProto {
    int32 threads_per_block_limit = 1;
    int32 threads_per_warp = 2;
    int32 shared_memory_per_block = 3;
    int32 shared_memory_per_core = 4;
    int32 threads_per_core_limit = 5;
    int32 core_count = 6;
    int64 fpus_per_core = 7;
    int32 block_dim_limit_x = 8;
    int32 block_dim_limit_y = 9;
    int32 block_dim_limit_z = 10;
    int64 memory_bandwidth = 11;
    int64 l2_cache_size = 12;
    float clock_rate_ghz = 13;
    int64 device_memory_size = 14;
    int32 shared_memory_per_block_optin = 15;
    int64 registers_per_core_limit = 18;
    int64 registers_per_block_limit = 19;
    
    oneof compute_capability {
        CudaComputeCapabilityProto cuda_compute_capability = 16;
        RocmComputeCapabilityProto rocm_compute_capability = 17;
    }
}

message DnnVersionInfoProto {
    int32 major = 1;
    int32 minor = 2;
    int32 patch = 3;
}

message GpuTargetConfigProto {
    GpuDeviceInfoProto gpu_device_info = 1;
    string platform_name = 4;
    DnnVersionInfoProto dnn_version_info = 5;
    .xla.AutotuneResults autotune_results = 6;
    string device_description_str = 7;
    
    reserved 2, 3, "cuda_compute_capability", "rocm_compute_capability";
}
